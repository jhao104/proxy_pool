# -*- coding: utf-8 -*-
# !/usr/bin/env python
"""
-------------------------------------------------
   File Nameï¼š     GetFreeProxy.py
   Description :  æŠ“å–å…è´¹ä»£ç†
   Author :       JHao
   dateï¼š          2016/11/25
-------------------------------------------------
   Change Activity:
                   2016/11/25:
-------------------------------------------------
"""
import re
import requests

try:
    from importlib import reload  # py3 å®é™…ä¸ä¼šå®ç”¨ï¼Œåªæ˜¯ä¸ºäº†ä¸æ˜¾ç¤ºè¯­æ³•é”™è¯¯
except:
    import sys  # py2

    reload(sys)
    sys.setdefaultencoding('utf-8')

from Util.utilFunction import robustCrawl, getHtmlTree
from Util.WebRequest import WebRequest

# for debug to disable insecureWarning
requests.packages.urllib3.disable_warnings()


class GetFreeProxy(object):
    """
    proxy getter
    """

    def __init__(self):
        pass

    @staticmethod
    @robustCrawl  # decoration print error if exception happen
    def freeProxyFirst(page=10):
        """
        æŠ“å–æ— å¿§ä»£ç† http://www.data5u.com/
        :param page: é¡µæ•°
        :return:
        """
        url_list = ['http://www.data5u.com/',
                    'http://www.data5u.com/free/',
                    'http://www.data5u.com/free/gngn/index.shtml',
                    'http://www.data5u.com/free/gnpt/index.shtml']
        for url in url_list:
            html_tree = getHtmlTree(url)
            ul_list = html_tree.xpath('//ul[@class="l2"]')
            for ul in ul_list:
                yield ':'.join(ul.xpath('.//li/text()')[0:2])

    @staticmethod
    @robustCrawl
    def freeProxySecond(proxy_number=100):
        """
        æŠ“å–ä»£ç†66 http://www.66ip.cn/
        :param proxy_number: ä»£ç†æ•°é‡
        :return:
        """
        url = "http://www.66ip.cn/mo.php?sxb=&tqsl={}&port=&export=&ktip=&sxa=&submit=%CC%E1++%C8%A1&textarea=".format(
            proxy_number)
        request = WebRequest()
        # html = request.get(url).content
        # contentä¸ºæœªè§£ç ï¼Œtextä¸ºè§£ç åçš„å­—ç¬¦ä¸²
        html = request.get(url).text
        for proxy in re.findall(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d{1,5}', html):
            yield proxy

    @staticmethod
    @robustCrawl
    def freeProxyThird(days=1):
        """
        æŠ“å–ip181 http://www.ip181.com/
        :param days:
        :return:
        """
        url = 'http://www.ip181.com/'
        html_tree = getHtmlTree(url)
        tr_list = html_tree.xpath('//tr')[1:]
        for tr in tr_list:
            yield ':'.join(tr.xpath('./td/text()')[0:2])

    @staticmethod
    @robustCrawl
    def freeProxyFourth():
        """
        æŠ“å–è¥¿åˆºä»£ç† http://api.xicidaili.com/free2016.txt
        :return:
        """
        url_list = ['http://www.xicidaili.com/nn',  # é«˜åŒ¿
                    'http://www.xicidaili.com/nt',  # é€æ˜
                    ]
        for each_url in url_list:
            tree = getHtmlTree(each_url)
            proxy_list = tree.xpath('.//table[@id="ip_list"]//tr')
            for proxy in proxy_list:
                yield ':'.join(proxy.xpath('./td/text()')[0:2])

    @staticmethod
    @robustCrawl
    def freeProxyFifth():
        """
        æŠ“å–guobanjia http://www.goubanjia.com/free/gngn/index.shtml
        :return:
        """
        url = "http://www.goubanjia.com/free/gngn/index{page}.shtml"
        for page in range(1, 10):
            page_url = url.format(page=page)
            tree = getHtmlTree(page_url)
            proxy_list = tree.xpath('//td[@class="ip"]')
            # æ­¤ç½‘ç«™æœ‰éšè—çš„æ•°å­—å¹²æ‰°ï¼Œæˆ–æŠ“å–åˆ°å¤šä½™çš„æ•°å­—æˆ–.ç¬¦å·
            # éœ€è¦è¿‡æ»¤æ‰<p style="display:none;">çš„å†…å®¹
            xpath_str = """.//*[not(contains(@style, 'display: none'))
                                and not(contains(@style, 'display:none'))
                                and not(contains(@class, 'port'))
                                ]/text()
                        """
            for each_proxy in proxy_list:
                # :ç¬¦å·è£¸æ”¾åœ¨tdä¸‹ï¼Œå…¶ä»–æ”¾åœ¨div span pä¸­ï¼Œå…ˆåˆ†å‰²æ‰¾å‡ºipï¼Œå†æ‰¾port
                ip_addr = ''.join(each_proxy.xpath(xpath_str))
                port = each_proxy.xpath(
                    ".//span[contains(@class, 'port')]/text()")[0]
                yield '{}:{}'.format(ip_addr, port)

    @staticmethod
    @robustCrawl
    def taiyangProxySix():
        """
        å¤ªé˜³ğŸŒä»£ç† http://www.taiyangruanjian.com/        
        """
        url = "http://http-api.taiyangruanjian.com/getip?num=5000&type=2&pro=0&city=0&yys=0&port=11&pack=2664&ts=1&ys=1&cs=1&lb=1&sb=1&pb=5&mr=3"
        headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Connection': 'keep-alive',
            'Accept-Language': 'zh-CN,zh;q=0.8',
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3212.0 Safari/537.36',
        }
        response = requests.get(url, headers=headers, timeout=30)
        if response.status_code is 200:
            jsonBody = response.json()
            if "data" in jsonBody and "code" in jsonBody and "success" in jsonBody:
                code = jsonBody["code"]
                if code is 0 and jsonBody["success"]:
                    data = jsonBody["data"]
                    for item in data:
                        yield "{}:{}".format(item["ip"], item["port"])
                else:
                    print jsonBody["msg"]


if __name__ == '__main__':
    gg = GetFreeProxy()
    # for e in gg.freeProxyFirst():
    #     print e

    # for e in gg.freeProxySecond():
    #     print e

    # for e in gg.freeProxyThird():
    #     print e

    # for e in gg.freeProxyFourth():
    #     print(e)

    for e in gg.freeProxyFifth():
        print(e)

    # for e in gg.taiyangProxySix():
    #     print(e)
